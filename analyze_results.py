#!/usr/bin/env python3
"""Analyze profile test results for interesting patterns."""

import json
import os
from collections import defaultdict

# Load all analysis files
analyses = {}
for filename in os.listdir('profile_analyses'):
    if filename.endswith('_analysis.json'):
        profile_id = filename.replace('_analysis.json', '')
        with open(f'profile_analyses/{filename}', 'r') as f:
            analyses[profile_id] = json.load(f)

print(f'ğŸ“Š Loaded {len(analyses)} profile analyses\n')

# Load test prompts for reference
with open('test_prompts.json', 'r') as f:
    test_prompts = {t['title']: t for t in json.load(f)}

# Aggregate statistics
total_ratings = sum(len(a.get('ratings', {})) for a in analyses.values())
print(f'Total ratings across all profiles: {total_ratings}\n')

# Per-profile summary
print('=' * 80)
print('PROFILE SUMMARIES')
print('=' * 80)

for profile_id, data in sorted(analyses.items()):
    ratings = data.get('ratings', {})
    if not ratings:
        continue
    
    label = data.get('profile_label', 'No label')
    
    # Count affinities
    affinities = [r.get('affinity') for r in ratings.values()]
    native = affinities.count('native_fit')
    workable = affinities.count('workable')
    resistant = affinities.count('resistant')
    
    # Average score
    scores = [r.get('score', 0) for r in ratings.values()]
    avg_score = sum(scores) / len(scores) if scores else 0
    
    print(f'\n{profile_id} - "{label}"')
    print(f'  Ratings: {len(ratings)}')
    print(f'  Affinities: âœ… {native} native | âš ï¸  {workable} workable | âŒ {resistant} resistant')
    print(f'  Avg Score: {avg_score:.1f}/10')

print('\n' + '=' * 80)
print('PROMPT/STYLE ANALYSIS')
print('=' * 80)

# Which prompts are most universally native_fit vs resistant?
prompt_affinities = defaultdict(lambda: {'native': 0, 'workable': 0, 'resistant': 0, 'profiles': []})

for profile_id, data in analyses.items():
    ratings = data.get('ratings', {})
    for prompt_title, rating in ratings.items():
        affinity = rating.get('affinity', 'unknown')
        if affinity == 'native_fit':
            prompt_affinities[prompt_title]['native'] += 1
        elif affinity == 'workable':
            prompt_affinities[prompt_title]['workable'] += 1
        elif affinity == 'resistant':
            prompt_affinities[prompt_title]['resistant'] += 1
        prompt_affinities[prompt_title]['profiles'].append(profile_id)

# Sort by native fit count
print('\nğŸ† Most Universally NATIVE prompts (high native_fit across profiles):')
sorted_prompts = sorted(prompt_affinities.items(), 
                       key=lambda x: (x[1]['native'], -x[1]['resistant']), 
                       reverse=True)
for i, (prompt, stats) in enumerate(sorted_prompts[:10], 1):
    total = stats['native'] + stats['workable'] + stats['resistant']
    if total > 0:
        native_pct = (stats['native'] / total) * 100
        print(f'{i:2}. {prompt[:60]:60} | âœ… {stats["native"]:2}/{total} ({native_pct:.0f}%) | âš ï¸  {stats["workable"]:2} | âŒ {stats["resistant"]:2}')

print('\nğŸ’€ Most Universally RESISTANT prompts (high resistance across profiles):')
sorted_resistant = sorted(prompt_affinities.items(), 
                         key=lambda x: (x[1]['resistant'], -x[1]['native']), 
                         reverse=True)
for i, (prompt, stats) in enumerate(sorted_resistant[:10], 1):
    total = stats['native'] + stats['workable'] + stats['resistant']
    if total > 0:
        resistant_pct = (stats['resistant'] / total) * 100
        print(f'{i:2}. {prompt[:60]:60} | âŒ {stats["resistant"]:2}/{total} ({resistant_pct:.0f}%) | âš ï¸  {stats["workable"]:2} | âœ… {stats["native"]:2}')

print('\n' + '=' * 80)
print('SCORE DISTRIBUTION')
print('=' * 80)

# Score distribution across all ratings
all_scores = []
for data in analyses.values():
    all_scores.extend([r.get('score', 0) for r in data.get('ratings', {}).values()])

if all_scores:
    print(f'\nOverall Statistics:')
    print(f'  Total Ratings: {len(all_scores)}')
    print(f'  Average Score: {sum(all_scores)/len(all_scores):.1f}/10')
    print(f'  Min Score: {min(all_scores)}')
    print(f'  Max Score: {max(all_scores)}')
    
    # Score histogram
    score_bins = defaultdict(int)
    for score in all_scores:
        bin_key = f'{score:.0f}'
        score_bins[bin_key] += 1
    
    print(f'\n  Score Distribution:')
    for score in range(1, 11):
        count = score_bins.get(str(score), 0)
        bar = 'â–ˆ' * (count // 5) + 'â–Œ' * (1 if count % 5 >= 3 else 0)
        print(f'    {score:2}/10: {count:3} {bar}')

print('\n' + '=' * 80)
print('STYLE CATEGORIES')
print('=' * 80)

# Analyze by category
category_stats = defaultdict(lambda: {'native': 0, 'workable': 0, 'resistant': 0, 'count': 0})

for prompt_title, prompt_data in test_prompts.items():
    category = prompt_data.get('category', 'Unknown')
    
    # Get affinity stats for this prompt across all profiles
    if prompt_title in prompt_affinities:
        stats = prompt_affinities[prompt_title]
        category_stats[category]['native'] += stats['native']
        category_stats[category]['workable'] += stats['workable']
        category_stats[category]['resistant'] += stats['resistant']
        category_stats[category]['count'] += 1

print('\nAffinity by Style Category:')
for category, stats in sorted(category_stats.items(), key=lambda x: x[1]['native'], reverse=True):
    total = stats['native'] + stats['workable'] + stats['resistant']
    if total > 0:
        native_pct = (stats['native'] / total) * 100
        print(f'{category:30} | âœ… {stats["native"]:3} ({native_pct:5.1f}%) | âš ï¸  {stats["workable"]:3} | âŒ {stats["resistant"]:3} | ({stats["count"]} prompts)')

print('\n' + '=' * 80)
print('KEY INSIGHTS')
print('=' * 80)

# Find most divisive prompts (high variance)
print('\nğŸ² Most DIVISIVE prompts (varying results across profiles):')
divisive_prompts = []
for prompt, stats in prompt_affinities.items():
    total = stats['native'] + stats['workable'] + stats['resistant']
    if total >= 3:  # At least 3 profiles tested
        # Calculate variance (simple metric: how spread out are the results?)
        variance = (stats['native'] * stats['resistant'])  # High when both native and resistant are high
        divisive_prompts.append((prompt, stats, variance))

divisive_prompts.sort(key=lambda x: x[2], reverse=True)
for i, (prompt, stats, variance) in enumerate(divisive_prompts[:10], 1):
    total = stats['native'] + stats['workable'] + stats['resistant']
    print(f'{i:2}. {prompt[:60]:60} | âœ… {stats["native"]:2} | âš ï¸  {stats["workable"]:2} | âŒ {stats["resistant"]:2}')

# Find consensus prompts (everyone agrees)
print('\nğŸ¤ Most CONSENSUS prompts (similar results across profiles):')
consensus_prompts = []
for prompt, stats in prompt_affinities.items():
    total = stats['native'] + stats['workable'] + stats['resistant']
    if total >= 3:
        # Consensus = one category dominates
        max_count = max(stats['native'], stats['workable'], stats['resistant'])
        consensus_score = max_count / total
        consensus_prompts.append((prompt, stats, consensus_score))

consensus_prompts.sort(key=lambda x: x[2], reverse=True)
for i, (prompt, stats, consensus) in enumerate(consensus_prompts[:10], 1):
    dominant = 'Native' if stats['native'] == max(stats['native'], stats['workable'], stats['resistant']) else ('Workable' if stats['workable'] > stats['resistant'] else 'Resistant')
    print(f'{i:2}. {prompt[:60]:60} | {dominant:9} ({consensus*100:.0f}% agree) | âœ… {stats["native"]:2} | âš ï¸  {stats["workable"]:2} | âŒ {stats["resistant"]:2}')

print('\n' + '=' * 80)
